{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora_question_similarity.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bmSDm5Q7cY0qpL9vOCDuYexSJMWFG72y",
      "authorship_tag": "ABX9TyPcI6BC3jF7YLGfo9v5/4/P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshgupta9723/Quora_question_scrappers/blob/main/Quora_question_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLEUglTiA8sK"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from scipy import spatial\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# # Downloading the stop words list\n",
        "nltk.download('stopwords')\n",
        "# # Loading the stop words in english\n",
        "stop_words = list(stopwords.words('english'))\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_data = pd.read_csv('/content/drive/MyDrive/tutree_work/related_q (1).csv')\n",
        "\n",
        "main_data.head()"
      ],
      "metadata": {
        "id": "2MvqBSl9S3SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def job_cleaner(text):\n",
        "    \"\"\"\n",
        "            text: a string\n",
        "            return: cleaned initial string\n",
        "        \"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "    text = str(text)\n",
        "    text = text.lower()  # lowercase text\n",
        "    # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text)\n",
        "    text = ' '.join(word for word in text.split()\n",
        "                    if word not in STOPWORDS)  # delete stopwords\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "nHr6ZitcLpnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_vector(text):\n",
        "    vector = []\n",
        "    vector.append(text.split(\" \"))\n",
        "    model = Word2Vec(vector, min_count=1, size=300)\n",
        "    # list of words\n",
        "    total_words = list(model.wv.vocab)\n",
        "    # adding\n",
        "    one_vector = []\n",
        "    for i in range(len(total_words)):\n",
        "        one_vector.append(model[total_words[i]])\n",
        "\n",
        "    sum_vector = sum(np.asarray(one_vector))\n",
        "    if (len(total_words) > 0):\n",
        "        feature_vec = np.divide(sum_vector, len(total_words))\n",
        "\n",
        "    return feature_vec"
      ],
      "metadata": {
        "id": "vaN_r3YtL1bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_list = main_data['Questions']\n",
        "\n",
        "vectors = []\n",
        "\n",
        "for question in question_list:\n",
        "    clean_text = job_cleaner(question)\n",
        "    vector = make_vector(clean_text)\n",
        "    vectors.append(vector)"
      ],
      "metadata": {
        "id": "01dZORXnW8R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similarity_score():\n",
        "\n",
        "    list_of_similar_ques = []\n",
        "\n",
        "    for question in question_list:\n",
        "        clean_text = job_cleaner(question)\n",
        "        vector = make_vector(clean_text)\n",
        "\n",
        "        similar_question = []\n",
        "\n",
        "        for i, ques in enumerate(question_list):\n",
        "            clean_text = job_cleaner(ques)\n",
        "            q_vector = make_vector(clean_text)\n",
        "\n",
        "            sim = 1 - spatial.distance.cosine(vector, q_vector)\n",
        "            if sim > 0.60:\n",
        "                similar_question.append(ques)\n",
        "\n",
        "        list_of_similar_ques.append(similar_question)\n",
        "\n",
        "    return list_of_similar_ques\n",
        "    \n",
        "final_list = find_similarity_score()"
      ],
      "metadata": {
        "id": "BzxkIIKtLGcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = pd.DataFrame()\n",
        "final_data['question'] = question_list\n",
        "final_data['similar_question'] = final_list\n",
        "\n",
        "final_data.to_csv(\"/content/drive/MyDrive/tutree_work/similar_questions.csv\")"
      ],
      "metadata": {
        "id": "bpeiKCIDDB1h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}